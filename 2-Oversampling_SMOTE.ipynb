{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adopted-mechanics",
   "metadata": {},
   "source": [
    "# Oversampling con SMOTE\n",
    "\n",
    "Provo ad applicare l'algoritmo SMOTE per fare oversampling e tentare di indurre nuovamente alberi di decisione, sempre di pronfondità massima 4.\n",
    "\n",
    "Ho utilizzato come riferimento una classe messa a disposizione da Stefano dell'Oca, riadattandola al mio task di classificazione multi-classe\n",
    "\n",
    "La classe da me realizzata incapsula l'applicazione di oversampling con SMOTE, SMOTENC (una variante di SMOTE applicabile in presenza di feature sia numeriche che categoriche), il training del modello selezionato e la cross validazione\n",
    "\n",
    "La classe permette inoltre di realizzare oversampling duplicando i campioni delle classi minoritarie nel dataset e applicando ai nuovi samples \"artificiali\" del rumore casuale. Nella classe sono settati i parametri che stabiliscono la probabilità con cui ad un sample sintetico è applicato del rumore e con cui una feature di un sample sintatico selezionato sarà oggetto di aggiunta di rumore o meno. Il rumore random che si applica alle feature appartiene sempre al range di variazione che caratterizza la feature nel dataset\n",
    "\n",
    "Ho implementato due versioni distinte della classe in questione: una in cui l'oversampling è applicato PRIMA di fare cross validazione e una in cui, dopo aver diviso i samples in folder, si applica l'oversampling alle folder da usare per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "numerical-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "#questa versione prima fa augmentation, poi fa cross val\n",
    "class SEC_aug_pre_split(object):\n",
    "    def __init__(self, X, Y, aug_method, learning_alg, n_fold, n_it, seed):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.aug_method = aug_method\n",
    "        self.learning_alg = learning_alg\n",
    "        self.n_fold = n_fold\n",
    "        self.n_it = n_it\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.accuracy_median_scores = np.zeros(self.n_it)\n",
    "        \n",
    "    #oversamplo tutte le classi\n",
    "    def random_oversample(self, X_train, Y_train):\n",
    "        ros = RandomOverSampler(random_state = self.seed)\n",
    "        X_train_aug, Y_train_aug = ros.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #roversamplo tutte le classi\n",
    "    def SMOTE_augmentation(self, X_train, Y_train):\n",
    "        smt = SMOTE(random_state = self.seed , k_neighbors = 2)\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #anche qua oresamplo tutte le classi\n",
    "    #questo va usato in caso di feature categoriche\n",
    "    def SMOTENC_augmentation(self, X_train, Y_train, cat_indexes):#assumo di non aver fatto one-hot-encoding prima\n",
    "        smt = SMOTENC(k_neighbors = 2, random_state=self.seed, categorical_features=cat_indexes)\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #oversampling con aggiunta di rumore random\n",
    "    def oversample_augmentation(self, X_train, Y_train, err_perc, prob_modifica):#qua assumo di non aver fatto one hot enconding\n",
    "        \n",
    "        #modifico il parametro in maniera opportuna\n",
    "        if err_perc > 1:\n",
    "            err_perc /= 100\n",
    "            \n",
    "        #ricavo range di ciascuna feature numerica\n",
    "        min_features = X_train.drop(['TIC type'], axis = 1).min()\n",
    "        max_features = X_train.drop(['TIC type'], axis = 1).max()\n",
    "        \n",
    "        #metà del range di variazione delle features numeriche\n",
    "        scales = (max_features-min_features)/2\n",
    "        \n",
    "        #numero di samples iniziali per ogni classe\n",
    "        n_samples_iniziali = Counter(Y_train)\n",
    "        \n",
    "        \n",
    "        #indice dal quale sono presenti i samples artificiali aggiunti\n",
    "        index_partenza_art = len(Y_train)\n",
    "        \n",
    "        #pareggio le  classi introducendo nuovi samples artificiali ottenuti duplicando quelli presenti\n",
    "        X_train_aug, Y_train_aug = self.random_oversample(X_train, Y_train)\n",
    "        \n",
    "        #salvo numero di samples aggiunti per ogni classe\n",
    "        n_samples_post_aug = Counter(Y_train_aug)\n",
    "        added_samples = n_samples_post_aug-n_samples_iniziali\n",
    "        \n",
    "        \n",
    "        #calcolo numero di samples artificiali a cui applicherò del rumore, in funzione di err_perc\n",
    "        n_apply_noise = dict()\n",
    "        for k in added_samples.keys():\n",
    "             n_apply_noise[k] = round(added_samples[k] * err_perc)\n",
    "                \n",
    "        \n",
    "        #ricavo gli indici dei samples artificiali delle varie classi a cui devo applicare rumore                                                                                               \n",
    "        indexes_1 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 1 and index >= index_partenza_art], n_apply_noise[1])\n",
    "        indexes_2 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 2 and index >= index_partenza_art], n_apply_noise[2])\n",
    "        #sample di classe 3 non vengono aggiunti, sono la classe maggioritaria\n",
    "        indexes_4 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 4 and index >= index_partenza_art], n_apply_noise[4])\n",
    "        indexes_5 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 5 and index >= index_partenza_art], n_apply_noise[5])\n",
    "       \n",
    "        indici_da_modificare = pd.Series([indexes_1, indexes_2, indexes_4,indexes_5])\n",
    "        indici_da_modificare.reindex([1,2,4,5])\n",
    "     \n",
    "        mu,sigma=0, 0.1\n",
    "        \n",
    "        for classe, indici in enumerate(indici_da_modificare):\n",
    "            for indice in indici:\n",
    "                for feature in X_train_aug.columns:\n",
    "                    n = random.uniform(0,1)\n",
    "                    if n < prob_modifica:\n",
    "                        if feature.startswith('TIC'):#la feature è categorica\n",
    "                            X_train_aug.at[indice, feature] = random.choice(X_train_aug['TIC type'].unique())\n",
    "                        else:#la feature è numerica\n",
    "                           \n",
    "                            X_train_aug.at[indice, feature] = round(X_train_aug.at[indice, feature]+scales[feature]*np.random.normal(mu, sigma))\n",
    "                            #verifico se é uscito dall'intervallo, in caso gli imposto il valore massimo                                        \n",
    "                            X_train_aug.at[indice, feature]=min(X_train_aug.at[indice, feature], max_features[feature])\n",
    "                            #verifico se è uscito dall'intervallo, in caso gli imposto il valore minimo\n",
    "                            X_train_aug.at[indice, feature]=max(X_train_aug.at[indice, feature], min_features[feature])\n",
    "\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #fa augmentation dei samples, fa cross validation e calcola l'accuratezza media per le varie iterazioni della cross-val\n",
    "    def fit_score(self):\n",
    "        for niter in range(self.n_it):\n",
    "            \n",
    "            #Qua prima augmento e poi faccio cross val classica  \n",
    "            \n",
    "            #Augmentation\n",
    "            if self.aug_method == 'smote':#da applicare solo se ho tutte feature numeriche, altrimenti uso smotec o oversample\n",
    "                X_train_aug, Y_train_aug = self.SMOTE_augmentation(self.X, self.Y)\n",
    "            elif self.aug_method == 'smotenc':\n",
    "                X_train_aug, Y_train_aug = self.SMOTENC_augmentation(self.X, self.Y, [3])\n",
    "                X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'] )\n",
    "            elif self.aug_method == 'oversample':\n",
    "                X_train_aug, Y_train_aug = self.oversample_augmentation(self.X, self.Y,err_perc=10, prob_modifica=0.7)\n",
    "                X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'] )\n",
    "            else:\n",
    "                X_train_aug, Y_train_aug = self.X.copy(), self.Y.copy()\n",
    "\n",
    "          \n",
    "            #Predictions\n",
    "            scores = cross_val_score(self.learning_alg, X_train_aug, Y_train_aug)#di default è 5 fold cross val\n",
    "\n",
    "            self.accuracy_median_scores[niter] = scores.mean()\n",
    "               \n",
    "    def print_result(self):\n",
    "        print(\"Accuratezza: %f\\n\" %(np.mean(self.accuracy_median_scores)))\n",
    "        \n",
    "    def get_tree_smotenc(self):\n",
    "        X_train_aug, Y_train_aug = self.SMOTENC_augmentation(self.X, self.Y, [3])\n",
    "        X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'] )\n",
    "        albero = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "        albero = albero.fit(X_train_aug, Y_train_aug)\n",
    "        data = tree.export_graphviz(albero, out_file=None, \n",
    "                      feature_names=X_train_aug.columns,  \n",
    "                      class_names=pd.Series(Y_train_aug.unique()).astype(str).sort_values().tolist(),  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True) \n",
    "        graph = graphviz.Source(data) \n",
    "        graph.render(\"alberoSMOTENC\")\n",
    "        \n",
    "    def get_tree_oversample(self):\n",
    "        X_train_aug, Y_train_aug = self.oversample_augmentation(self.X, self.Y, err_perc=10, prob_modifica=0.7)\n",
    "        X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'] )\n",
    "        albero = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "        albero = albero.fit(X_train_aug, Y_train_aug)\n",
    "        data = tree.export_graphviz(albero, out_file=None, \n",
    "                      feature_names=X_train_aug.columns,  \n",
    "                      class_names=pd.Series(Y_train_aug.unique()).astype(str).sort_values().tolist(),  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True) \n",
    "        graph = graphviz.Source(data) \n",
    "        graph.render(\"alberoOversample\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "taken-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "#versione che fa cross val prima dell'augmentazione\n",
    "\n",
    "#usa KFold e non cross_val_score\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class SEC_aug_post_split(object):\n",
    "    def __init__(self, X, Y, aug_method, learning_alg, n_fold, n_it, seed):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.aug_method = aug_method\n",
    "        self.learning_alg = learning_alg\n",
    "        self.n_fold = n_fold\n",
    "        self.n_it = n_it\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.accuracy_median_scores = np.zeros(self.n_it)\n",
    "        \n",
    "    #oversamplo tutte le classi\n",
    "    def random_oversample(self, X_train, Y_train):\n",
    "        ros = RandomOverSampler(random_state = self.seed)\n",
    "        X_train_aug, Y_train_aug = ros.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #oversamplo tutte le classi\n",
    "    def SMOTE_augmentation(self, X_train, Y_train):\n",
    "        smt = SMOTE(random_state = self.seed , k_neighbors = 2)\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #anche qua resamplo tutte le classi\n",
    "    #questo va usato in caso di feature categoriche\n",
    "    def SMOTENC_augmentation(self, X_train, Y_train, cat_indexes):#assumo di non aver ancora fatto one-hot-encoding\n",
    "        smt = SMOTENC(k_neighbors = 2, random_state=self.seed, categorical_features=cat_indexes)\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #oversampling con agiunta rumore random\n",
    "    def oversample_augmentation(self, X_train, Y_train, err_perc, prob_modifica):\n",
    "        \n",
    "        if err_perc > 1:\n",
    "            err_perc /= 100\n",
    "        \n",
    "        #ricavo range di ciascuna feature\n",
    "        min_features = X_train.drop(['TIC type'], axis = 1).min()\n",
    "        max_features = X_train.drop(['TIC type'], axis = 1).max()\n",
    "        \n",
    "         #metà del range\n",
    "        scales = (max_features-min_features)/2\n",
    "        \n",
    "        #numero di samples iniziali per ogni classe\n",
    "        n_samples_iniziali = Counter(Y_train)\n",
    "        \n",
    "        #indice dal quale sono presenti i samples artificiali aggiunti\n",
    "        index_partenza_art = len(Y_train)\n",
    "        \n",
    "        #pareggio le  classi introducendo nuovi samples artificiali\n",
    "        X_train_aug, Y_train_aug = self.random_oversample(X_train, Y_train)\n",
    "        \n",
    "        #salvo numero di samples aggiunti per ogni classe\n",
    "        n_samples_post_aug = Counter(Y_train_aug)\n",
    "        added_samples = n_samples_post_aug-n_samples_iniziali\n",
    "        \n",
    "        #calcolo numero di samples artificiali a cui applicherò del rumore, in funzione di err_perc\n",
    "        n_apply_noise = dict()\n",
    "        for k in added_samples.keys():\n",
    "             n_apply_noise[k] = round(added_samples[k] * err_perc)\n",
    "                \n",
    "        #ricavo gli indici dei samples artificiali delle varie classi a cui devo applicare rumore                                                     \n",
    "        indexes_1 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 1 and index >= index_partenza_art], n_apply_noise[1])\n",
    "        indexes_2 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 2 and index >= index_partenza_art], n_apply_noise[2])\n",
    "        indexes_4 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 4 and index >= index_partenza_art], n_apply_noise[4])\n",
    "        indexes_5 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 5 and index >= index_partenza_art], n_apply_noise[5])\n",
    "       \n",
    "        indici_da_modificare = pd.Series([indexes_1, indexes_2, indexes_4,indexes_5])\n",
    "        indici_da_modificare.reindex([1,2,4,5])\n",
    "     \n",
    "        mu,sigma=0, 0.1\n",
    "        \n",
    "        \n",
    "        for classe, indici in enumerate(indici_da_modificare):\n",
    "            for indice in indici:\n",
    "                for feature in X_train_aug.columns:\n",
    "                    n = random.uniform(0,1)\n",
    "                    if n < prob_modifica:\n",
    "                        if feature.startswith('TIC'):#la feature è categorica\n",
    "                            X_train_aug.at[indice, feature] = random.choice(X_train_aug['TIC type'].unique())\n",
    "                        else:#la feature è numerica\n",
    "                            \n",
    "                            X_train_aug.at[indice, feature] = round(X_train_aug.at[indice, feature]+scales[feature]*np.random.normal(mu, sigma))\n",
    "                            #verifico che se fosse uscito dall'intervallo gli imposto il valore massimo                                        \n",
    "                            X_train_aug.at[indice, feature]=min(X_train_aug.at[indice, feature], max_features[feature])\n",
    "                            #verifico che se fosse uscito dall'intervallo gli imposto il valore minimo\n",
    "                            X_train_aug.at[indice, feature]=max(X_train_aug.at[indice, feature], min_features[feature])\n",
    "\n",
    "        return X_train_aug, Y_train_aug\n",
    "    #prima crea le folder per cross-val, poi augmenta i sample del training set, poi fitta il modello e poi calcola accuratezza\n",
    "    def fit_score(self):\n",
    "        for niter in range(self.n_it):\n",
    "            \n",
    "            #e setto che fa shuffling\n",
    "            sf = KFold(n_splits = self.n_fold, shuffle = True, random_state = self.seed)\n",
    "            \n",
    "            accuracy_scores = []\n",
    "           \n",
    "            for train_index, test_index in sf.split(self.X, self.Y):\n",
    "                X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "                Y_train, Y_test = self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "              \n",
    "                #Augmentation\n",
    "                if self.aug_method == 'smote':\n",
    "                    X_train_aug, Y_train_aug = self.SMOTE_augmentation(X_train, Y_train)\n",
    "                elif self.aug_method == 'smotenc':\n",
    "                    X_train_aug, Y_train_aug = self.SMOTENC_augmentation(X_train, Y_train, [3])\n",
    "                    X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'])\n",
    "                elif self.aug_method == 'oversample':\n",
    "                    X_train_aug, Y_train_aug = self.oversample_augmentation(X_train, Y_train,err_perc=10, prob_modifica=0.7)\n",
    "                    X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'])\n",
    "                else:\n",
    "                    X_train_aug, Y_train_aug = X_train, Y_train\n",
    "                    X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'])\n",
    "                \n",
    "                #Training\n",
    "                clf = self.learning_alg.fit(X_train_aug, Y_train_aug)\n",
    "                \n",
    "                #Predictions\n",
    "                X_test = pd.get_dummies(X_test, columns = ['TIC type'])\n",
    "                columns = X_test.columns\n",
    "                if('TIC type_A' not in columns):\n",
    "                    X_test['TIC type_A'] = 0\n",
    "                if('TIC type_B'not in columns):\n",
    "                    X_test['TIC type_B']= 0\n",
    "                if('TIC type_C'not in columns):\n",
    "                    X_test['TIC type_C'] = 0\n",
    "                if('TIC type_D'not in columns):\n",
    "                    X_test['TIC type_D'] =0\n",
    "               \n",
    "                predictions = clf.predict(X_test)\n",
    "                \n",
    "                #calcolo accuracy\n",
    "                accuracy_scores.append(accuracy_score(predictions, Y_test))\n",
    "                \n",
    "                \n",
    "            self.accuracy_median_scores[niter] = np.mean(accuracy_scores)\n",
    "        #sarebbe carino salvare file pdf del'albero\n",
    "            \n",
    "\n",
    "    def print_result(self):\n",
    "        print(\"Accuratezza: %f\\n\" %(np.mean(self.accuracy_median_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inner-pasta",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eta</th>\n",
       "      <th>Segni macro malignità</th>\n",
       "      <th>ADC</th>\n",
       "      <th>TIC type</th>\n",
       "      <th>T2</th>\n",
       "      <th>COD ISTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2.44</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     eta  Segni macro malignità   ADC TIC type  T2  COD ISTO\n",
       "0     33                      0  2.60        D   0         1\n",
       "1     50                      0  2.44        D   0         1\n",
       "2     81                      1  0.70        C   1         1\n",
       "3     28                      0  1.60        A   1         1\n",
       "4     51                      0  1.40        A   0         1\n",
       "..   ...                    ...   ...      ...  ..       ...\n",
       "102   27                      0  1.30        C   1         1\n",
       "103   50                      0  1.90        A   0         5\n",
       "104   62                      0  2.10        A   0         5\n",
       "105   37                      0  1.80        A   0         5\n",
       "106   42                      0  1.10        A   0         5\n",
       "\n",
       "[107 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importo il ds\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Db_parotide_Def_REV_Samuel.xlsx')\n",
    "df['ADC'] = df['ADC'].apply(lambda s: str(s).replace(',', '.')).astype(float)\n",
    "df['Output Algoritmo'].iloc[0] = 3\n",
    "df['Output Algoritmo'].iloc[1] = 3\n",
    "df['Output Algoritmo'].iloc[20] = 3\n",
    "dfAlbero = df.iloc[:, 1:8]\n",
    "dfAlbero = dfAlbero.drop(['Output Algoritmo'], axis = 1)\n",
    "dfAlbero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "checked-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.458222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = dfAlbero['COD ISTO']\n",
    "\n",
    "from sklearn import tree\n",
    "albero = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4 , min_samples_leaf = 2)\n",
    "\n",
    "seed = 42\n",
    "prova = SEC_aug_pre_split(X,Y, 'smotenc', albero, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiac-image",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.496593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prova = SEC_aug_pre_split(X,Y, 'oversample', albero, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "upset-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.253680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prova = SEC_aug_post_split(X,Y, 'smotenc', albero, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "active-attitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.179766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prova = SEC_aug_post_split(X,Y, 'oversample', albero, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nasty-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fare oversampling prima del training e fittare il modello sui dati augmentati sembra la strategia che conduce alla\n",
    "#maggiore accuratezza\n",
    "#tuttavia non sono sicuro che questo sia l'approccio giusto\n",
    "#ora provo a visualizzare l'albero che si ottiene facendo training su tutto il dataset augmentato con smote e \n",
    "#con oversample augmentation\n",
    "\n",
    "prova = SEC_aug_pre_split(X,Y, 'oversample', albero, 5, 50, seed)\n",
    "prova.get_tree_oversample()\n",
    "prova.get_tree_smotenc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
