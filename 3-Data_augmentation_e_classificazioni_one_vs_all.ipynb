{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "killing-hospital",
   "metadata": {},
   "source": [
    "# Obbiettivi\n",
    "\n",
    "Precedentemente ho stimato l'accuratezza degli alberi di decisione indotti da training set sottoposti ad augmentation con SMOTE o attraverso l'aggiunta di rumore casuale, per classificare i tumori nelle 5 categorie individuate (la 7 è stata eliminata dal ds e i tumori di classe 6 sono stati incorporati nella classe 1). I risultati non erano ottimali: l'accuratezza non superava il 20% e le regole che gli alberi utilizzavano per la classificaizione tenevano quasi solo conto di età e ADC.\n",
    "Provo ora un approccio differente. Per ognuna delle 5 classi induco (usando augmentatiom, sia con smote che con il rumore casuale) un albero che classifica i samples come appartenenti a quella classe o meno (cioè realizza una classificazione binaria) e ne valuto le prestazioni. Infine darò le propabilità di classificazione fornite da ciascuno di questi alberi in pasto ad un ulteriore albero di decisione e ad una random forest di massimo 10 alberi, sempre con e senza data augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respected-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eta</th>\n",
       "      <th>Segni macro malignità</th>\n",
       "      <th>ADC</th>\n",
       "      <th>TIC type</th>\n",
       "      <th>T2</th>\n",
       "      <th>COD ISTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2.44</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     eta  Segni macro malignità   ADC TIC type  T2  COD ISTO\n",
       "0     33                      0  2.60        D   0         1\n",
       "1     50                      0  2.44        D   0         1\n",
       "2     81                      1  0.70        C   1         1\n",
       "3     28                      0  1.60        A   1         1\n",
       "4     51                      0  1.40        A   0         1\n",
       "..   ...                    ...   ...      ...  ..       ...\n",
       "102   27                      0  1.30        C   1         1\n",
       "103   50                      0  1.90        A   0         5\n",
       "104   62                      0  2.10        A   0         5\n",
       "105   37                      0  1.80        A   0         5\n",
       "106   42                      0  1.10        A   0         5\n",
       "\n",
       "[107 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importo il ds\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Db_parotide_Def_REV_Samuel.xlsx')\n",
    "df['ADC'] = df['ADC'].apply(lambda s: str(s).replace(',', '.')).astype(float)\n",
    "df['Output Algoritmo'].iloc[0] = 3\n",
    "df['Output Algoritmo'].iloc[1] = 3\n",
    "df['Output Algoritmo'].iloc[20] = 3\n",
    "dfAlbero = df.iloc[:, 1:8]\n",
    "dfAlbero = dfAlbero.drop(['Output Algoritmo'], axis = 1)\n",
    "dfAlbero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "published-sculpture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    54\n",
       "4    27\n",
       "1    16\n",
       "2     5\n",
       "5     5\n",
       "Name: COD ISTO, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do un ulteriore sguardo alle classi minoritatrie nel ds\n",
    "dfAlbero['COD ISTO'].value_counts()\n",
    "#Penso di quadruplicarele 2 classi minoritarie col rumore random e con smotenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "assumed-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creo i samples e le labels\n",
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = dfAlbero['COD ISTO']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-consultancy",
   "metadata": {},
   "source": [
    "## Innanzitutto rifinisco la classe che avevo implementato nel task precedente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "latter-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per ogni cross val, l'augmentation è fatta solo sul training set, il test set è lasciato immacolato \n",
    "#ad ogni interazione\n",
    "#usa KFold e non cross_val_score\n",
    "\n",
    "#Questa classe serve per determinare l'accuratezza di un modello per la classificazione multi classe\n",
    "#rispetto a dei dati \n",
    "#agumentanti con smote o col rumore random\n",
    "\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class SEC_aug(object):\n",
    "    def __init__(self, X, Y, aug_method, learning_alg, n_fold, n_it, seed):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.aug_method = aug_method\n",
    "        self.learning_alg = learning_alg\n",
    "        self.n_fold = n_fold\n",
    "        self.n_it = n_it\n",
    "        self.seed = seed\n",
    "        self.accuracy_median_scores = []\n",
    "    #oversamplo tutte le classi tranne quella maggioritataria duplicando samples scelti randomicamente\n",
    "    def random_oversample_raw(self, X_train, Y_train):\n",
    "        ros = RandomOverSampler(random_state = self.seed)\n",
    "        X_train_aug, Y_train_aug = ros.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #oversample solo le classi specificate in diz, secondo quanto specificato in diz\n",
    "    def random_oversample_dict(self, X_train, Y_train, diz):\n",
    "        ros = RandomOverSampler(sampling_strategy = diz, random_state = self.seed)\n",
    "        X_train_aug, Y_train_aug = ros.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #oversamplo tutte le classi non maggioritatarie con SMOTE-->solo feature numeriche!!\n",
    "    #Ho dovuto ridurre il k_neighbors a 2 perchè nei training set infividuati con la cross val\n",
    "    #talvolta, sopratutto per le due classi minoritarie 2 e 5, vi è un numero limitato di samples\n",
    "    #In ogni caso ho imposto nella cross validation di fare augmentation e calcolo accuratezza\n",
    "    #solo negli split in cui il training set contiene almeno 2 rappresentanti per la classe 2 e 5\n",
    "    #in quanto ritengo che avendo a diposizione solo 1 o meno samples da cui fare interpolazione \n",
    "    #non porti ad una buona augmentation\n",
    "    \n",
    "    #ciò significa anche che dovrò fare un elevato numero di iterazioni\n",
    "    def SMOTE_augmentation_raw(self, X_train, Y_train):\n",
    "        smt = SMOTE(random_state = self.seed , k_neighbors = 1)#questo perchè in alcuni split vi sono pochissimi rappresentanti di certe classi\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #oversamplo tutte le classi specificate nel dizionario-->solo feature numeriche!!\n",
    "    def SMOTE_augmentation_dict(self, X_train, Y_train, diz):\n",
    "        smt = SMOTE(sampling_strategy = diz, random_state = self.seed , k_neighbors = 1)# a causa di scarso numero rappresentanti delle classi minoritarie nelle fold\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #anche qua resamplo tutte le classi-->per feature sia chategoriche che numeriche!!\n",
    "    def SMOTENC_augmentation_raw(self, X_train, Y_train, cat_indexes):#assumo di non aver ancora fatto one-hot-encoding\n",
    "        smt = SMOTENC(k_neighbors = 1, random_state=self.seed, categorical_features=cat_indexes)\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #anche qua resamplo le classi indicate dal dizionario-->per feature sia categoriche che numeriche!!\n",
    "    def SMOTENC_augmentation_dict(self, X_train, Y_train, cat_indexes, diz):#assumo di non aver ancora fatto one-hot-encoding\n",
    "        smt = SMOTENC(k_neighbors = 2, random_state=self.seed, categorical_features=cat_indexes, sampling_strategy = diz)\n",
    "        X_train_aug, Y_train_aug = smt.fit_resample(X_train, Y_train)\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #oversampling con aggiunta rumore random-->quadruplico i samples delle sole 2 classi minoritarie\n",
    "    #err_perc --> la probabilità che un sample artificiale venga sottoposto ad aggiunta di rumore\n",
    "    #prob_modifica --> la probabilità che il valore di una feature di un sample artificiale a cui\n",
    "    #applicare rumore venga modificato\n",
    "    def oversample_augmentation(self, X_train, Y_train, err_perc, prob_modifica):\n",
    "        \n",
    "        if err_perc > 1:\n",
    "            err_perc /= 100\n",
    "        \n",
    "        #ricavo range di ciascuna feature numerica\n",
    "        min_features = X_train.drop(['TIC type'], axis = 1).min()\n",
    "        max_features = X_train.drop(['TIC type'], axis = 1).max()\n",
    "        \n",
    "        #metà del range per ciascuna feature numerica\n",
    "        scales = (max_features-min_features)/2\n",
    "        \n",
    "        #numero di samples iniziali per ogni classe\n",
    "        n_samples_iniziali = Counter(Y_train)\n",
    "        \n",
    "        #indice dal quale sono aggiunti i samples artificiali\n",
    "        index_partenza_art = len(Y_train)\n",
    "        \n",
    "        #pareggio le  classi introducendo nuovi samples artificiali\n",
    "        \n",
    "        diz = {2: n_samples_iniziali[2]*4, 5: n_samples_iniziali[5]*4}#quadruplo le 2 classi minoritarie\n",
    "        #per il momento il ratio è hard coded, ma potrei modificare la funzione per adattarla \n",
    "        #anche a ds diversi. Per ora non ne sento la necessità, devo solo triplicare due classi specifiche\n",
    "        X_train_aug, Y_train_aug = self.random_oversample_dict(X_train, Y_train, diz)\n",
    "        \n",
    "        #salvo numero di samples aggiunti per ogni classe\n",
    "        n_samples_post_aug = Counter(Y_train_aug)\n",
    "        added_samples = n_samples_post_aug-n_samples_iniziali\n",
    "        \n",
    "        #calcolo numero di samples artificiali a cui applicherò del rumore, in funzione di err_perc\n",
    "        n_apply_noise = dict()\n",
    "        for k in added_samples.keys():\n",
    "             n_apply_noise[k] = round(added_samples[k] * err_perc)\n",
    "                \n",
    "        #ricavo gli indici dei samples artificiali delle varie classi a cui devo applicare rumore                                                     \n",
    "        \n",
    "        indexes_2 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 2 and index >= index_partenza_art], n_apply_noise[2])\n",
    "        indexes_5 = random.sample([index for index in Y_train_aug.index if Y_train_aug[index] == 5 and index >= index_partenza_art], n_apply_noise[5])\n",
    "       \n",
    "        indici_da_modificare = pd.Series([indexes_2,indexes_5])\n",
    "        indici_da_modificare.reindex([2,5])\n",
    "        #sempre hard coded, ma per ora mi va bene\n",
    "     \n",
    "        mu,sigma=0, 0.1\n",
    "        \n",
    "        \n",
    "        for classe, indici in enumerate(indici_da_modificare):\n",
    "            for indice in indici:\n",
    "                for feature in X_train_aug.columns:\n",
    "                    n = random.uniform(0,1)\n",
    "                    if n < prob_modifica:\n",
    "                        if feature.startswith('TIC'):#la feature è categorica\n",
    "                            X_train_aug.at[indice, feature] = random.choice(X_train_aug['TIC type'].unique())\n",
    "                        else:#la feature è numerica\n",
    "                            X_train_aug.at[indice, feature] = round(X_train_aug.at[indice, feature]+scales[feature]*np.random.normal(mu, sigma))\n",
    "                            #verifico che se fosse uscito dall'intervallo gli imposto il valore massimo                                        \n",
    "                            X_train_aug.at[indice, feature]=min(X_train_aug.at[indice, feature], max_features[feature])\n",
    "                            #verifico che se fosse uscito dall'intervallo gli imposto il valore minimo\n",
    "                            X_train_aug.at[indice, feature]=max(X_train_aug.at[indice, feature], min_features[feature])\n",
    "\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #prima crea le folder per cross-val, poi augmenta i sample del training set, poi fitta il modello e poi calcola accuratezza\n",
    "    def fit_score(self):\n",
    "        for niter in range(self.n_it):\n",
    "            #e setto che fa shuffling\n",
    "            #random_state = self.seed rimosso, perchè credo che se lo setto ad ogni iterazione dà lo stesso split\n",
    "            #ogni volta, invece io vorrei avere folder diverse ad ogni interazione\n",
    "            #Potrei pensare anche di usare RepeatedKFold\n",
    "            \n",
    "            sf = KFold(n_splits = self.n_fold, shuffle = True)\n",
    "            \n",
    "            accuracy_scores = []\n",
    "           \n",
    "            for train_index, test_index in sf.split(self.X, self.Y):\n",
    "                X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "                Y_train, Y_test = self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "                \n",
    "                \n",
    "                if Y_train.value_counts()[2] <= 1 or Y_train.value_counts()[5] <= 1:#in caso contrario non è possibile applicare SMOTE\n",
    "                    continue\n",
    "    \n",
    "                diz = dict()#anche questo è hard coded\n",
    "                diz[2] = 4*len([x for x in Y_train if x == 2])\n",
    "                diz[5] = 4*len([x for x in Y_train if x == 5])\n",
    "                \n",
    "            \n",
    "                #Augmentation\n",
    "                #Anche qua per ora ho degli elementi hard coded\n",
    "                #Sostanzialmente faccio che o oversamplo tutte le classi tranne la maggioritaria (raw)\n",
    "                #oppure quadruplico le due classi minoritarie nel ds (non raw)\n",
    "                if self.aug_method == 'smote':\n",
    "                    X_train_aug, Y_train_aug = self.SMOTE_augmentation_dict(X_train, Y_train, diz)\n",
    "               \n",
    "                elif self.aug_method == 'smote_raw':\n",
    "                    X_train_aug, Y_train_aug = self.SMOTE_augmentation_raw(X_train, Y_train)\n",
    " \n",
    "                elif self.aug_method == 'smotenc':\n",
    "                    X_train_aug, Y_train_aug = self.SMOTENC_augmentation_dict(X_train, Y_train, [3], diz)\n",
    "\n",
    "                elif self.aug_method == 'smotenc_raw':\n",
    "                    X_train_aug, Y_train_aug = self.SMOTENC_augmentation_raw(X_train, Y_train, [3])\n",
    " \n",
    "                elif self.aug_method == 'oversample':#ci sono due parametri hard coded\n",
    "                    X_train_aug, Y_train_aug = self.oversample_augmentation(X_train, Y_train,err_perc=75, prob_modifica=0.25)\n",
    "\n",
    "                else:#no data augmentation\n",
    "                    X_train_aug, Y_train_aug = X_train, Y_train\n",
    "                    \n",
    "                X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'])\n",
    "                #Training\n",
    "                \n",
    "                #Per gestire casi in cui nel training set non siano presenti sample con certi tipi di TIC\n",
    "                columns = X_train_aug.columns\n",
    "                if('TIC type_A' not in columns):\n",
    "                    X_train_aug['TIC type_A'] = 0\n",
    "                if('TIC type_B'not in columns):\n",
    "                    X_train_aug['TIC type_B']= 0\n",
    "                if('TIC type_C'not in columns):\n",
    "                    X_train_aug['TIC type_C'] = 0\n",
    "                if('TIC type_D'not in columns):\n",
    "                    X_train_aug['TIC type_D'] =0\n",
    "                \n",
    "                \n",
    "                clf = self.learning_alg.fit(X_train_aug, Y_train_aug)\n",
    "                \n",
    "                #Predictions\n",
    "                #Per gestire il fatto che nel test set potrebbero non esserci samples con una certa curva TIC\n",
    "                X_test = pd.get_dummies(X_test, columns = ['TIC type'])\n",
    "                columns = X_test.columns\n",
    "                if('TIC type_A' not in columns):\n",
    "                    X_test['TIC type_A'] = 0\n",
    "                if('TIC type_B'not in columns):\n",
    "                    X_test['TIC type_B']= 0\n",
    "                if('TIC type_C'not in columns):\n",
    "                    X_test['TIC type_C'] = 0\n",
    "                if('TIC type_D'not in columns):\n",
    "                    X_test['TIC type_D'] =0\n",
    "                    \n",
    "                    \n",
    "                predictions = clf.predict(X_test)\n",
    "                \n",
    "                #calcolo accuracy\n",
    "                accuracy_scores.append(accuracy_score(predictions, Y_test))\n",
    "                \n",
    "                \n",
    "            self.accuracy_median_scores.append( np.mean(accuracy_scores))\n",
    "            \n",
    "    def print_result(self):\n",
    "        print(\"Accuratezza: %f\\n\" %(np.mean(self.accuracy_median_scores)))\n",
    "        self.accuracy_median_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "exempt-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.315506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OK, la classe per l'augmentation per la classificazione multi classe è stata definita\n",
    "#Faccio una prova\n",
    "\n",
    "#Accuratezza usando l'introduzione di rumore random per l'augmentation\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "seed = 42\n",
    "\n",
    "alberoProva = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_aug(X,Y, 'oversample', alberoProva, 5, 50, seed )\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "located-excellence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.331423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuratezza usando smotenc dove quadruplo solo le 2 classi minoritarie\n",
    "\n",
    "alberoProva = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_aug(X,Y, 'smotenc', alberoProva, 5, 50, seed )\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "other-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.252530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuratezza usando smotenc_raw dove oversamplo tutte le classi non minoratarie\n",
    "\n",
    "alberoProva = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_aug(X,Y, 'smotenc_raw', alberoProva, 5, 50, seed )\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-township",
   "metadata": {},
   "source": [
    "## Ora induco gli alberi per le classificazioni binarie sopra menzionate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "supported-physics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eta</th>\n",
       "      <th>Segni macro malignità</th>\n",
       "      <th>ADC</th>\n",
       "      <th>T2</th>\n",
       "      <th>TIC type_A</th>\n",
       "      <th>TIC type_B</th>\n",
       "      <th>TIC type_C</th>\n",
       "      <th>TIC type_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     eta  Segni macro malignità   ADC  T2  TIC type_A  TIC type_B  TIC type_C  \\\n",
       "0     33                      0  2.60   0           0           0           0   \n",
       "1     50                      0  2.44   0           0           0           0   \n",
       "2     81                      1  0.70   1           0           0           1   \n",
       "3     28                      0  1.60   1           1           0           0   \n",
       "4     51                      0  1.40   0           1           0           0   \n",
       "..   ...                    ...   ...  ..         ...         ...         ...   \n",
       "102   27                      0  1.30   1           0           0           1   \n",
       "103   50                      0  1.90   0           1           0           0   \n",
       "104   62                      0  2.10   0           1           0           0   \n",
       "105   37                      0  1.80   0           1           0           0   \n",
       "106   42                      0  1.10   0           1           0           0   \n",
       "\n",
       "     TIC type_D  \n",
       "0             1  \n",
       "1             1  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "..          ...  \n",
       "102           0  \n",
       "103           0  \n",
       "104           0  \n",
       "105           0  \n",
       "106           0  \n",
       "\n",
       "[107 rows x 8 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Albero per la classificazione 1 vs 2,3,4,5\n",
    "\n",
    "#1 indica che appartiene alla classe specificata\n",
    "#0 indica che non vi appartiene\n",
    "\n",
    "dfAlbero1 = dfAlbero.copy()\n",
    "Y = [1 if x == 1 else 0 for x in dfAlbero1['COD ISTO'] ]\n",
    "X = dfAlbero1.iloc[:, :-1]\n",
    "\n",
    "\n",
    "X_encoded = pd.get_dummies(X, columns = ['TIC type'])\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "endless-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.832035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#ho 16 samples della classe 1, quindi magari l'oversample non è necessario per avere un buon albero\n",
    "#faccio una cv a 5 fold veloce sull'intero ds non augmentato\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "albero1 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "scores = cross_val_score(albero1, X_encoded, Y)\n",
    "print(\"Accuratezza: %f\\n\" %scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "opened-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.773     1.000     0.872        17\n",
      "           1      0.000     0.000     0.000         5\n",
      "\n",
      "    accuracy                          0.773        22\n",
      "   macro avg      0.386     0.500     0.436        22\n",
      "weighted avg      0.597     0.773     0.674        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Raccolgo qualche dato in più\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_encoded,Y, test_size = 0.2, random_state = seed)\n",
    "albero1 = albero1.fit(X_train, Y_train)\n",
    "predictions = albero1.predict(X_test)\n",
    "\n",
    "print(predictions)\n",
    "print(classification_report(Y_test, predictions, digits = 3))\n",
    "\n",
    "#Non va bene, l'albero classifica tutti gli input come non 1 e l'accuratezza è elevata a causa di pochi samples 1\n",
    "#L'augmentation SERVE dunque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "veterinary-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe per l'Augmentation per le classificazioni binarie\n",
    "#Qua l'oversampling porta il numero di samples della classe minoritaria al numero di samples di \n",
    "#quella maggioritaria\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, recall_score, precision_score, matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "class SEC_bin(object):\n",
    "    def __init__(self, X, Y, data_aug_method, learning_alg, n_fold, n_it, seed):\n",
    "        self.X = X\n",
    "        self.Y=Y\n",
    "        self.data_aug_method = data_aug_method\n",
    "        self.learning_alg = learning_alg\n",
    "        self.n_fold = n_fold\n",
    "        self.n_it = n_it\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.spec=np.zeros(self.n_it)\n",
    "        self.sens = np.zeros(self.n_it)\n",
    "        self.f1=np.zeros(self.n_it)\n",
    "        self.acc=np.zeros(self.n_it)\n",
    "        self.prec=np.zeros(self.n_it)\n",
    "        self.matt =np.zeros(self.n_it)\n",
    "        self.cohen=np.zeros(self.n_it)  \n",
    "\n",
    "    #data augmentation se non sono presenti variabili categoriche (Quindi usa SMOTE)\n",
    "    def SMOTE_augmentation(self, X_train, Y_train):\n",
    "    \n",
    "        sm=SMOTE(random_state=self.seed)\n",
    "        X_train_aug, Y_train_aug = sm.fit_resample(X_train, Y_train)\n",
    "\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #data augmentation se sono presenti variabili categoriche (Quindi usa SMOTENC)\n",
    "    def SMOTENC_augmentation_cat(self, X_train, Y_train, cat_indexes):\n",
    "    \n",
    "        sm=SMOTENC(random_state=self.seed, categorical_features=cat_indexes, k_neighbors = 5)\n",
    "        X_train_aug, Y_train_aug = sm.fit_resample(X_train, Y_train)\n",
    "\n",
    "        return X_train_aug, Y_train_aug\n",
    "    \n",
    "    #data augmentation con oversample\n",
    "    def oversample_augmentation(self, X_train, Y_train, err_perc, prob_modifica): #err_perc è la percentuale di quelli che avranno l'errore nella data aug.\n",
    "        #conta quante volte compare 1 e quante 0 nell'elemtno passato tra ().\n",
    "        counter = Counter(Y_train) \n",
    "   \n",
    "        #così so quanti sono gli elementi a cui dovrò mettere un errore.\n",
    "        missing=counter[0]-counter[1]\n",
    " \n",
    "        #creo un indice da cui so che i successivi elementi sono stati aggiunti\n",
    "        partenza=counter[0]+counter[1] \n",
    "   \n",
    "        #prendo il valore min e max per ogni feature numerica\n",
    "        min_feat = X_train.drop(['TIC type'], axis = 1).min()\n",
    "        max_feat = X_train.drop(['TIC type'], axis = 1).max()\n",
    "        \n",
    "        #prendo la metà dell'intervallo per ogni feat. numerica. Se mod_prob=1 allora shifto di mezzo intervallo.\n",
    "        scales=(max_feat-min_feat)/2 \n",
    "        \n",
    "        #verifico la percentuale inserita nell'argomento\n",
    "        if  err_perc>1:\n",
    "            err_perc=err_perc/100\n",
    "        \n",
    "        #pareggio le due classi\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority', random_state = self.seed)\n",
    "    \n",
    "        X_train_aug, Y_train_aug = oversample.fit_resample(X_train, Y_train)\n",
    "        \n",
    "        #numero di quelli che avranno l'errore modificare (quelli che voglio aggiungere*%). Potrebbe non essere intero\n",
    "        n_modificate=missing*err_perc\n",
    "        #ora è intero: round approssima all'intero più vicino\n",
    "        n_modificate=round(n_modificate) \n",
    "        #andrò a prendere n_modificate alla fine di X_train_selected_aug, andando a vedere le ultime \"missing\".\n",
    "        \n",
    "#mi salvo gli indici dei valori a cui devo andare ad aggiugere l'errore (l'errore viene aggiunto solo ai valori \"nuovi\")\n",
    "        indici_prescelti= random.sample(range(partenza, partenza+missing), n_modificate)\n",
    "        \n",
    "        \n",
    "        features = X_train_aug.columns                                                                                \n",
    "\n",
    "        mu,sigma=0, 0.1\n",
    "    #itero indici e features. E vado poi a verificare quanto vale prob_mod per vedere se il valore viene modificato e, se si, di quanto\n",
    "    #quindi scorro le righe e dopo le colonne    \n",
    "        features = X_train_aug.columns\n",
    "        for indice in indici_prescelti:\n",
    "            \n",
    "            for feature in features:\n",
    "                \n",
    "                #tre casi per le possibili feature che posso trovare\n",
    "                if abs(random.uniform(0,1))< prob_modifica:\n",
    "                    if feature.startswith('TIC'):\n",
    "                        \n",
    "                        #vado a calcolare il modulo della probabilità di modifica: se è >sigma (68%?) allora ne inverto il valore\n",
    "                        X_train_aug.at[indice, feature] = random.choice(X_train_aug['TIC type'].unique())\n",
    "               \n",
    "                    else :\n",
    "                        X_train_aug.at[indice, feature] = round(X_train_aug.at[indice, feature]+scales[feature]*np.random.normal(mu, sigma))\n",
    "                        #verifico che se fosse uscito dall'intervallo gli imposto il valore massimo                                        \n",
    "                        X_train_aug.at[indice, feature]=min(X_train_aug.at[indice, feature], max_feat[feature])\n",
    "                        #verifico che se fosse uscito dall'intervallo gli imposto il valore minimo\n",
    "                        X_train_aug.at[indice, feature]=max(X_train_aug.at[indice, feature], min_feat[feature])\n",
    "\n",
    "\n",
    "        return X_train_aug, Y_train_aug\n",
    "\n",
    "    def fit_score(self):\n",
    "        y_pred=np.zeros(self.Y.shape[0])\n",
    "        \n",
    "        #n iterazioni\n",
    "        for niter in range(self.n_it):\n",
    "            \n",
    "            skf = StratifiedKFold(n_splits=self.n_fold)\n",
    "            \n",
    "            #outerfold\n",
    "            for train_index, test_index in skf.split(self.X, self.Y): \n",
    "                                    \n",
    "                X_train, X_test = self.X.iloc[train_index,:], self.X.iloc[test_index,:]\n",
    "\n",
    "                Y_train, Y_test = self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "                \n",
    "# DATA AUGMENTATION\n",
    "                \n",
    "                if self.data_aug_method=='smote':\n",
    "        \n",
    "                    #array che viene creato ogni volta per \n",
    "                    cat_indexes = [3]#hard coded\n",
    "                    #divido in categorici e numerici. i lo uso come indice, feature mi indica la i-esima feature \n",
    "                   \n",
    "                    if cat_indexes:\n",
    "                        X_train_aug, Y_train_aug = self.SMOTENC_augmentation_cat(X_train, Y_train, cat_indexes)\n",
    "                      \n",
    "                    else:\n",
    "                        X_train_aug, Y_train_aug = self.SMOTE_augmentation(X_train, Y_train)\n",
    "                      \n",
    "                elif self.data_aug_method=='oversample':                                                                                                           \n",
    "                    X_train_aug, Y_train_aug = self.oversample_augmentation(X_train, Y_train, err_perc=75, prob_modifica=0.25)\n",
    "                else:\n",
    "                    X_train_aug, Y_train_aug = X_train, Y_train\n",
    "                    #return \n",
    "##TRAIN\n",
    "                X_train_aug = pd.get_dummies(X_train_aug, columns = ['TIC type'])\n",
    "                #Training\n",
    "                columns = X_train_aug.columns\n",
    "                if('TIC type_A' not in columns):\n",
    "                    X_train_aug['TIC type_A'] = 0\n",
    "                if('TIC type_B'not in columns):\n",
    "                    X_train_aug['TIC type_B']= 0\n",
    "                if('TIC type_C'not in columns):\n",
    "                    X_train_aug['TIC type_C'] = 0\n",
    "                if('TIC type_D'not in columns):\n",
    "                    X_train_aug['TIC type_D'] =0\n",
    "                \n",
    "                \n",
    "                clf = self.learning_alg.fit(X_train_aug, Y_train_aug)\n",
    "    \n",
    "##TEST\n",
    "                X_test = pd.get_dummies(X_test, columns = ['TIC type'])\n",
    "                columns = X_test.columns\n",
    "                if('TIC type_A' not in columns):\n",
    "                    X_test['TIC type_A'] = 0\n",
    "                if('TIC type_B'not in columns):\n",
    "                    X_test['TIC type_B']= 0\n",
    "                if('TIC type_C'not in columns):\n",
    "                    X_test['TIC type_C'] = 0\n",
    "                if('TIC type_D'not in columns):\n",
    "                    X_test['TIC type_D'] =0\n",
    "                    \n",
    "                y_pred[test_index] = clf.predict(X_test)\n",
    "              \n",
    "        #fine ciclo for train_index, test_index in skf.split(X, Y):\n",
    "\n",
    "        # QUI ho le predizioni ottenute con questo split del dataset\n",
    "        #mi salvo il valore di ogni indice statistico, nell'array. Dovrò farne la media.\n",
    "            self.f1[niter]=f1_score(y_pred,self.Y)\n",
    "            self.sens[niter] = recall_score(y_pred,self.Y)\n",
    "            self.acc[niter]=accuracy_score(y_pred,self.Y)\n",
    "            self.prec[niter]=precision_score(y_pred,self.Y)\n",
    "            self.matt[niter]=matthews_corrcoef(y_pred,self.Y)\n",
    "            self.cohen[niter]=cohen_kappa_score(y_pred,self.Y)\n",
    "            \n",
    "\n",
    "    def print_result(self):\n",
    "        print(\"f1: %f\\n\" %(np.mean(self.f1)))\n",
    "        print(\"Sensibilità: %f\\n\" %(np.mean(self.sens)))\n",
    "        print(\"Accuratezza: %f\\n\" %(np.mean(self.acc)))\n",
    "        print(\"Precisione: %f\\n\" %(np.mean(self.prec)))\n",
    "        print(\"Matthews: %f\\n\" %(np.mean(self.matt)))\n",
    "        print(\"Kappa di Cohen: %f\\n\" %(np.mean(self.cohen)))\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-intake",
   "metadata": {},
   "source": [
    "# Induzione alberi per le classificazioni binarie\n",
    "## Albero 1 vs Altri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "spanish-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ora che ho definito la classe per l'augmentation nella classificazione binaria\n",
    "#osservo come le performance cambiano utilizzando data augmentation smote\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "undefined-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.295142\n",
      "\n",
      "Sensibilità: 0.222702\n",
      "\n",
      "Accuratezza: 0.687477\n",
      "\n",
      "Precisione: 0.437500\n",
      "\n",
      "Matthews: 0.132281\n",
      "\n",
      "Kappa di Cohen: 0.120911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1 vs altri con smote\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 1 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "albero1 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'smote', albero1, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "premier-marble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.244438\n",
      "\n",
      "Sensibilità: 0.187533\n",
      "\n",
      "Accuratezza: 0.672897\n",
      "\n",
      "Precisione: 0.353750\n",
      "\n",
      "Matthews: 0.065978\n",
      "\n",
      "Kappa di Cohen: 0.061075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ora 1 vs Altri con il rumore random (75,25)\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 1 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "albero1 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'oversample', albero1, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-ambassador",
   "metadata": {},
   "source": [
    "### Probabilità 1 vs altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cognitive-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.892     1.000     0.943        91\n",
      "           1      1.000     0.312     0.476        16\n",
      "\n",
      "    accuracy                          0.897       107\n",
      "   macro avg      0.946     0.656     0.710       107\n",
      "weighted avg      0.908     0.897     0.873       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I risultati sembrano comparabili\n",
    "#induco ora l'albero 1 vs 2,3,4,5 allenato su tutto il ds facendo augmentation con smote(nc)\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 1 else 0 for x in dfAlbero['COD ISTO']])\n",
    "albero1 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "sm=SMOTENC(random_state=seed, categorical_features=[3])#k_neighbors lasciato a default (5)\n",
    "X_aug, Y_aug = sm.fit_resample(X, Y)\n",
    "X_aug = pd.get_dummies(X_aug, columns = ['TIC type'])\n",
    "X = pd.get_dummies(X, columns = ['TIC type'])\n",
    "albero1 = albero1.fit(X_aug, Y_aug)\n",
    "predictions = albero1.predict(X)\n",
    "print(classification_report(Y, predictions, digits = 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "suited-competition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.        , 1.        ],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.65151515, 0.34848485],\n",
       "       [0.65151515, 0.34848485]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Non male\n",
    "#ora mi prendo le probabilità 1 vs altri\n",
    "prob1 = albero1.predict_proba(X)\n",
    "prob1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-complement",
   "metadata": {},
   "source": [
    "## Albero 2 vs Altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "charming-consequence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.055556\n",
      "\n",
      "Sensibilità: 0.032258\n",
      "\n",
      "Accuratezza: 0.682243\n",
      "\n",
      "Precisione: 0.200000\n",
      "\n",
      "Matthews: -0.043789\n",
      "\n",
      "Kappa di Cohen: -0.027103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ora valuto anche se per 2 vs altri smote e rumore random sono metodi di oversampling comparabili\n",
    "#Ho dovuto ridurre il parametro k_neighbors a 3 a causa di scarso numero rappresentanti\n",
    "\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 2 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "albero2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'smote', albero2, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "orange-glucose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.031013\n",
      "\n",
      "Sensibilità: 0.019127\n",
      "\n",
      "Accuratezza: 0.756449\n",
      "\n",
      "Precisione: 0.084000\n",
      "\n",
      "Matthews: -0.066214\n",
      "\n",
      "Kappa di Cohen: -0.048455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ora provo random noise\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 2 else 0 for x in dfAlbero['COD ISTO']])\n",
    "albero2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'oversample', albero2, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random noise qua sembra essere leggermente più accurato, ma per consistenza uso smote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-homework",
   "metadata": {},
   "source": [
    "### Probabilità 2 vs Altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fixed-pharmacology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.973     0.716     0.825       102\n",
      "           1      0.094     0.600     0.162         5\n",
      "\n",
      "    accuracy                          0.710       107\n",
      "   macro avg      0.534     0.658     0.494       107\n",
      "weighted avg      0.932     0.710     0.794       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2 vs altri\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 2 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "albero2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "sm=SMOTENC(random_state=seed, categorical_features=[3], k_neighbors = 4)#k_neighbors abbassato per pochi rappresentanti\n",
    "X_aug, Y_aug = sm.fit_resample(X, Y)\n",
    "\n",
    "X_aug = pd.get_dummies(X_aug, columns = ['TIC type'])\n",
    "X = pd.get_dummies(X, columns = ['TIC type'])\n",
    "albero2 = albero2.fit(X_aug, Y_aug)\n",
    "predictions = albero2.predict(X)\n",
    "print(classification_report(Y, predictions, digits = 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "united-making",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.33333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.10810811, 0.89189189],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38181818, 0.61818182],\n",
       "       [0.38181818, 0.61818182]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2 = albero2.predict_proba(X)\n",
    "prob2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-making",
   "metadata": {},
   "source": [
    "## Albero 3 vs Altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "geological-balloon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.425532\n",
      "\n",
      "Sensibilità: 0.500000\n",
      "\n",
      "Accuratezza: 0.495327\n",
      "\n",
      "Precisione: 0.370370\n",
      "\n",
      "Matthews: -0.007222\n",
      "\n",
      "Kappa di Cohen: -0.006971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ora valuto smote vs random noise per la classificazione dei tumori classe 3\n",
    "\n",
    "#classe 3 è maggioritaria-->costituisce metà dataset-->provo solo augmentation con smote\n",
    "\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 3 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "albero3 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'smote', albero3, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bizarre-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.470588\n",
      "\n",
      "Sensibilità: 0.500000\n",
      "\n",
      "Accuratezza: 0.495327\n",
      "\n",
      "Precisione: 0.444444\n",
      "\n",
      "Matthews: -0.008430\n",
      "\n",
      "Kappa di Cohen: -0.008377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#provo senza augmentation\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 3 else 0 for x in dfAlbero['COD ISTO']])\n",
    "albero3 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'nulla', albero3, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()\n",
    "\n",
    "#poca differenza tra smote e nessuna agmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-pillow",
   "metadata": {},
   "source": [
    "### Probabilità 3 vs Altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "violent-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.711     0.604     0.653        53\n",
      "           1      0.661     0.759     0.707        54\n",
      "\n",
      "    accuracy                          0.682       107\n",
      "   macro avg      0.686     0.682     0.680       107\n",
      "weighted avg      0.686     0.682     0.680       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3 vs altri\n",
    "#per consistenza anche qua applico smote alla classe minoritaria(in questo caso 0)\n",
    "X = dfAlbero.iloc[:, :-1].copy()\n",
    "Y = pd.Series([1 if x == 3 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "\n",
    "albero3 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "sm=SMOTENC(random_state=seed, categorical_features=[3])#k_neighbors lasciato a default (5)\n",
    "X_aug, Y_aug = sm.fit_resample(X, Y)\n",
    "\n",
    "X_aug = pd.get_dummies(X_aug, columns = ['TIC type'])\n",
    "X = pd.get_dummies(X, columns = ['TIC type'])\n",
    "albero3 = albero3.fit(X_aug, Y_aug)\n",
    "predictions = albero3.predict(X)\n",
    "print(classification_report(Y, predictions, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "private-brain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54545455, 0.45454545],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.48      , 0.52      ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.48      , 0.52      ],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.54545455, 0.45454545],\n",
       "       [0.42857143, 0.57142857],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42857143, 0.57142857]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob3 = albero3.predict_proba(X)\n",
    "prob3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-defeat",
   "metadata": {},
   "source": [
    "### Albero 4 vs Altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "antique-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.246628\n",
      "\n",
      "Sensibilità: 0.214198\n",
      "\n",
      "Accuratezza: 0.551776\n",
      "\n",
      "Precisione: 0.291111\n",
      "\n",
      "Matthews: -0.063324\n",
      "\n",
      "Kappa di Cohen: -0.061815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ora valuto 4 vs altri smote\n",
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = pd.Series([1 if x == 4 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "\n",
    "albero4 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'smote', albero4, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cellular-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.181682\n",
      "\n",
      "Sensibilità: 0.167151\n",
      "\n",
      "Accuratezza: 0.548972\n",
      "\n",
      "Precisione: 0.202963\n",
      "\n",
      "Matthews: -0.125741\n",
      "\n",
      "Kappa di Cohen: -0.124306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ora provo random noise \n",
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = pd.Series([1 if x == 4 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "albero4 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'oversample', albero4, 5, 50, seed)\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-highway",
   "metadata": {},
   "source": [
    "### Probabilità 4 vs Altri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "forbidden-chicago",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.903     0.700     0.789        80\n",
      "           1      0.467     0.778     0.583        27\n",
      "\n",
      "    accuracy                          0.720       107\n",
      "   macro avg      0.685     0.739     0.686       107\n",
      "weighted avg      0.793     0.720     0.737       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4 vs altri\n",
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = pd.Series([1 if x == 4 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "\n",
    "albero4 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "sm=SMOTENC(random_state=seed, categorical_features=[3])#k_neighbors lasciato a default (5)\n",
    "X_aug, Y_aug = sm.fit_resample(X, Y)\n",
    "\n",
    "X_aug = pd.get_dummies(X_aug, columns = ['TIC type'])\n",
    "X = pd.get_dummies(X, columns = ['TIC type'])\n",
    "albero4 = albero4.fit(X_aug, Y_aug)\n",
    "predictions = albero4.predict(X)\n",
    "print(classification_report(Y, predictions, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bigger-plenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7       , 0.3       ],\n",
       "       [0.7       , 0.3       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.7       , 0.3       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.4       , 0.6       ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.7       , 0.3       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.4       , 0.6       ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.7       , 0.3       ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.4       , 0.6       ],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [0.4       , 0.6       ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.        , 1.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.21428571, 0.78571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.33333333, 0.66666667]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob4 = albero4.predict_proba(X)\n",
    "prob4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-contest",
   "metadata": {},
   "source": [
    "## Albero 5 vs Altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "treated-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.085145\n",
      "\n",
      "Sensibilità: 0.054094\n",
      "\n",
      "Accuratezza: 0.799065\n",
      "\n",
      "Precisione: 0.200000\n",
      "\n",
      "Matthews: 0.015901\n",
      "\n",
      "Kappa di Cohen: 0.012499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ora valuto 5 vs altri smote \n",
    "#porto k_neighbors a 3 per scarso numero di rappresentanti\n",
    "\n",
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = pd.Series([1 if x == 5 else 0 for x in dfAlbero['COD ISTO']])\n",
    "\n",
    "albero5 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'smote', albero5, 5, 50, seed)#ho abbassato momentaneamente il numero di neighbors a 3\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "valued-flexibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.053828\n",
      "\n",
      "Sensibilità: 0.031022\n",
      "\n",
      "Accuratezza: 0.667103\n",
      "\n",
      "Precisione: 0.204000\n",
      "\n",
      "Matthews: -0.048861\n",
      "\n",
      "Kappa di Cohen: -0.029582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5 vs tutti augmentation con random noise\n",
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = pd.Series([1 if x == 5 else 0 for x in dfAlbero['COD ISTO']])\n",
    "albero5 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "prova = SEC_bin(X,Y, 'oversample', albero5, 5, 50, seed)#ho abbassato momentaneamente il numero di neighbors\n",
    "prova.fit_score()\n",
    "prova.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-lemon",
   "metadata": {},
   "source": [
    "### Probabilità 5 vs Altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "overhead-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.988     0.814     0.892       102\n",
      "           1      0.174     0.800     0.286         5\n",
      "\n",
      "    accuracy                          0.813       107\n",
      "   macro avg      0.581     0.807     0.589       107\n",
      "weighted avg      0.950     0.813     0.864       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = dfAlbero.iloc[:,:-1]\n",
    "Y = pd.Series([1 if x == 5 else 0 for x in dfAlbero['COD ISTO']])\n",
    "albero5 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "sm=SMOTENC(random_state=seed, categorical_features=[3], k_neighbors = 4)#k_neighbors abbassato per pochi rappresentanti\n",
    "X_aug, Y_aug = sm.fit_resample(X, Y)\n",
    "\n",
    "X_aug = pd.get_dummies(X_aug, columns = ['TIC type'])\n",
    "X = pd.get_dummies(X, columns = ['TIC type'])\n",
    "albero5 = albero5.fit(X_aug, Y_aug)\n",
    "predictions = albero5.predict(X)\n",
    "print(classification_report(Y, predictions, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "blocked-coordination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [1.        , 0.        ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [0.0990099 , 0.9009901 ],\n",
       "       [0.0990099 , 0.9009901 ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob5 = albero5.predict_proba(X)\n",
    "prob5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "premier-highlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.667     0.250     0.364        16\n",
      "           2      0.000     0.000     0.000         5\n",
      "           3      0.667     0.963     0.788        54\n",
      "           4      0.810     0.630     0.708        27\n",
      "           5      1.000     0.400     0.571         5\n",
      "\n",
      "    accuracy                          0.701       107\n",
      "   macro avg      0.629     0.449     0.486       107\n",
      "weighted avg      0.687     0.701     0.657       107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sam/anaconda3/envs/tirocinio/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#ora devo far confluire le probabilità fornite dai 5 alberi in un unico albero di decisione\n",
    "#provo, ma non sono sicuro di aver capito bene come dovrei indurre un albero di decisione dalle probabilità\n",
    "\n",
    "\n",
    "#creo samples e label\n",
    "Y = dfAlbero['COD ISTO'].copy()\n",
    "X = pd.DataFrame()\n",
    "\n",
    "X['Prob1'] = np.transpose(prob1)[1]\n",
    "X['Prob2'] = np.transpose(prob2)[1]\n",
    "X['Prob3'] = np.transpose(prob3)[1]\n",
    "X['Prob4'] = np.transpose(prob4)[1]\n",
    "X['Prob5'] = np.transpose(prob5)[1]\n",
    "\n",
    "#alleno sull'intero dataset, senza augmentation\n",
    "alberoComposto = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "alberoComposto.fit(X,Y)\n",
    "predictions1 = alberoComposto.predict(X)\n",
    "print(classification_report(Y, predictions1, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-belfast",
   "metadata": {},
   "source": [
    "## Problema con calcolo accuratezza nel caso dell'augmentation\n",
    "Se la suddivisione in folder della cross val precede l'augmentation con smote, in molti casi il training set non contiene rappresentanti delle classi minoritarie che devo sottoporre a cross validation\n",
    "\n",
    "Quindi o ignoro le folder che non contengono un numero opportuno di samples delle classi da augmentare (caso sotto), oppure non faccio la cross validation per misurare l'accuratezza, ma un generico split iterato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "worth-editing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.515873015873016"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Provo a fare augmentation con SMOTE delle due classi minoritarie, triplicandole\n",
    "#Valuto accuratezza della cross validation, ignorando le folder che non contengono samples delle classi minoritarie\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sm = SMOTE(sampling_strategy = {2:15,5:15}, random_state = seed, k_neighbors = 1)\n",
    "X_aug, Y_aug = sm.fit_resample(X,Y)\n",
    "alberoComposto = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "accuracy_median_scores = []\n",
    "for niter in range(100):\n",
    "    \n",
    "    \n",
    "    skf = KFold(n_splits=5)\n",
    "    \n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, Y): \n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        \n",
    "        if  2 not in Y_train.value_counts().index or 5 not in Y_train.value_counts().index or Y_train.value_counts()[2]<=1 or Y_train.value_counts()[5]<=1:\n",
    "            continue\n",
    "            \n",
    "        X_train_aug, Y_train_aug = sm.fit_resample(X_train, Y_train)\n",
    "        \n",
    "        alberodummy = albero1.fit(X_train_aug, Y_train_aug)\n",
    "        \n",
    "        predictions = alberodummy.predict(X_test)\n",
    "        \n",
    "        accuracy_scores.append(accuracy_score(predictions, Y_test))\n",
    "        \n",
    "    accuracy_median_scores.append( np.mean(accuracy_scores))\n",
    "\n",
    "np.mean(accuracy_median_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "medieval-escape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.487012987012987"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Valuto accuratezza splittando a caso, senza fare cross val\n",
    "\n",
    "sm = SMOTE(sampling_strategy = {2:15,5:15}, random_state = seed, k_neighbors = 1)\n",
    "\n",
    "alberoComposto = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "accuracy_scores = []\n",
    "for niter in range(50):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "\n",
    "    if  2 not in Y_train.value_counts().index or 5 not in Y_train.value_counts().index or Y_train.value_counts()[2]<=1 or Y_train.value_counts()[5]<=1:\n",
    "            continue\n",
    "    X_train_aug, Y_train_aug = sm.fit_resample(X_train, Y_train)\n",
    "    alberodummy = alberoComposto.fit(X_train_aug, Y_train_aug)\n",
    "    predictions = alberodummy.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(predictions, Y_test))\n",
    "        \n",
    "    \n",
    "\n",
    "np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "patent-youth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5849230769230769"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ora provo a indurre l'albero allenandolo sull'intero ds augmentato (smote che triplica le due classi minoritarie)\n",
    "\n",
    "sm = SMOTE(sampling_strategy = {2:15,5:15}, random_state = seed, k_neighbors = 1)\n",
    "\n",
    "alberoComposto = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "X_aug, Y_aug = sm.fit_resample(X,Y)\n",
    "scores = cross_val_score(alberoComposto, X_aug, Y_aug)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "unique-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alberoComposto.pdf'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ora visuzlizzo l'albero\n",
    "\n",
    "import graphviz\n",
    "\n",
    "sm = SMOTE(sampling_strategy = {2:15,5:15}, random_state = seed, k_neighbors = 1)\n",
    "\n",
    "alberoComposto = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, min_samples_leaf = 2)\n",
    "\n",
    "X_aug, Y_aug = sm.fit_resample(X,Y)\n",
    "alberoComposto = alberoComposto.fit(X_aug, Y_aug)\n",
    "\n",
    "data = tree.export_graphviz(alberoComposto, out_file=None, \n",
    "                      feature_names=X.columns,  \n",
    "                      class_names=pd.Series(Y.unique()).astype(str).sort_values().tolist(),  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True) \n",
    "graph = graphviz.Source(data) \n",
    "graph.render(\"alberoComposto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-copyright",
   "metadata": {},
   "source": [
    "# Riassunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "professional-madness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob1</th>\n",
       "      <th>Prob2</th>\n",
       "      <th>Prob3</th>\n",
       "      <th>Prob4</th>\n",
       "      <th>Prob5</th>\n",
       "      <th>COD ISTO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prob1     Prob2     Prob3     Prob4     Prob5  COD ISTO\n",
       "0    1.000000  0.333333  0.454545  0.300000  0.000000         1\n",
       "1    1.000000  0.333333  0.571429  0.300000  0.000000         1\n",
       "2    0.454545  0.000000  0.520000  0.000000  0.000000         1\n",
       "3    0.348485  0.618182  0.454545  0.230769  0.000000         1\n",
       "4    0.348485  0.618182  0.571429  0.666667  0.900990         1\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "102  0.047619  0.000000  0.454545  0.230769  0.000000         1\n",
       "103  0.348485  0.618182  0.571429  0.666667  0.900990         5\n",
       "104  0.300000  0.000000  0.333333  0.000000  0.526316         5\n",
       "105  0.348485  0.618182  0.000000  0.666667  0.900990         5\n",
       "106  0.348485  0.618182  0.571429  0.666667  0.900990         5\n",
       "\n",
       "[107 rows x 6 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['COD ISTO'] = Y\n",
    "X\n",
    "#Ogni feature rappresenta la probabilità che un sample apprtenga alla rispettiva classe piuttosto che alle altre 4\n",
    "#secondo quanto ricavato dagli alberi per la classificazione binaria"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
